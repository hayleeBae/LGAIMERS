{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dataset & Test Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import platform\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0번 GPU에 할당\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':2048,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "\n",
    "PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "train_enc = pd.read_csv(PATH + 'train_fe.csv', low_memory=False)\n",
    "sales = pd.read_csv(PATH + 'sales.csv')\n",
    "origin_train = pd.read_csv(PATH + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩\n",
    "\n",
    "col_names = ['대분류', '중분류', '소분류', '브랜드', '쇼핑몰', 'day_name']\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in col_names:\n",
    "    train_enc[col] = le.fit_transform(train_enc[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_idx = train_enc.query(\"event != '0'\").index\n",
    "train_enc.loc[event_idx, 'event'] = [1] * len(event_idx)\n",
    "train_enc = train_enc.sort_values(by = ['ID', 'date']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc = train_enc.drop('price', axis = 1)\n",
    "train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ID', '대분류', '중분류', '소분류', '브랜드', '쇼핑몰', 'day_name', 'quarter', 'keyword', 'event', 'sales', 'date']\n",
    "train_enc = train_enc[columns]\n",
    "train_enc = train_enc.drop('event', axis = 1)\n",
    "train_enc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train_enc.iloc[:4623360, :]\n",
    "train_2 = train_enc.iloc[4623360:9246240, :]\n",
    "train_3 = train_enc.iloc[9246240:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Train & Test Dataset to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  시간이 너무 오래걸림\n",
    "## 현실적으로 쓸 수 없는 함수\n",
    "## pandas 라이브러리가 너무 무거운 거로 판단됨 -> numpy 데이터로 변형 후 함수 적용하는게 맞는듯\n",
    "## numpy로 바꿔도 차이가 없음 : 시간복잡도가 너무 높아서 생기는 문제로 판단됨 O(n^2)\n",
    "## pandas.DataFrame.query()의 문제라고 판명\n",
    "## ID및 date의 순서로 되어있기 때문에 iloc을 사용해 순서대로 잘라서 dataset을 만들어서 작업 소요시간이 매우 줄음\n",
    "\n",
    "def make_train_data(data, train_size = CFG['TRAIN_WINDOW_SIZE'], predict_size = CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : date를 melt시킨 새로운 train data\n",
    "    train_size : 학습에 활용할 기간 => 90 Days\n",
    "    predict_size : 추론할 기간 => 21 Days\n",
    "    '''\n",
    "    window_size = train_size + predict_size         # 90 + 21 = 111\n",
    "    num_id = data.ID.nunique()                      # ID: 28894\n",
    "    num_date = data.date.nunique()                  # 날짜: 479\n",
    "    num_features = len(data.iloc[0, 1:10])           # date, sales를 제외한 나머지 features : 대분류 ~ event / sales <- Target\n",
    "    data = np.array(data)                           # DataFrame to Numpy Data\n",
    "    \n",
    "    input_data = np.empty((num_id * ((num_date + num_features) - window_size + 1), train_size, num_features + 1), dtype = np.float16)\n",
    "    target_data = np.empty((num_id * ((num_date + num_features) - window_size + 1), predict_size), dtype = np.float16)\n",
    "\n",
    "    for id in tqdm(range(0, num_id, 2)):\n",
    "        for j in range(num_date - window_size + 1):\n",
    "            temp_data = data[id*479: 479*(id+1)][j:train_size+j, 1:11]\n",
    "            input_data[id * ((num_date + num_features) - window_size + 1) + j] = temp_data\n",
    "            target_data[id * ((num_date + num_features) - window_size + 1) + j] = data[id*479: 479*(id+1)][train_size+j:window_size+j, 10] # sales\n",
    "\n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']): #90\n",
    "    '''\n",
    "    평가 데이터(Test Dataset)를 추론하기 위한 Input 데이터를 생성\n",
    "    data : date를 melt시킨 새로운 train data\n",
    "    train_size : 추론을 위해 필요한 일별 판매량 기간 (= 학습에 활용할 기간)\n",
    "    '''\n",
    "    num_id = data.ID.nunique()                      # ID: 15890\n",
    "    num_date = data.date.nunique()                  # 날짜: 425\n",
    "    num_features = len(data.iloc[0, 1:10])           # date를 제외한 나머지 features : 대분류 ~ sales / sales <- Target\n",
    "    data = np.array(data)\n",
    "    \n",
    "    test_input = np.empty((num_id, train_size, num_features + 1), dtype = np.float16)\n",
    "\n",
    "    for id in tqdm(range(num_id)):\n",
    "        temp_data = data[id*425: 425*(id+1)][-train_size:, 1:11]\n",
    "        test_input[id] = temp_data\n",
    "\n",
    "    return test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1, target_1 = make_train_data(train_1)\n",
    "test_1 = make_predict_data(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2, target_2 = make_train_data(train_2)\n",
    "test_2 = make_predict_data(train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3, target_3 = make_train_data(train_3)\n",
    "test_3 = make_predict_data(train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH + 'dataset/train_input_1', input_1)\n",
    "np.save(PATH + 'dataset/train_target_1', target_1)\n",
    "np.save(PATH + 'dataset/test_input_1', test_1)\n",
    "\n",
    "np.save(PATH + 'dataset/train_input_2', input_2)\n",
    "np.save(PATH + 'dataset/train_target_2', target_2)\n",
    "np.save(PATH + 'dataset/test_input_2', test_2)\n",
    "\n",
    "np.save(PATH + 'dataset/train_input_3', input_3)\n",
    "np.save(PATH + 'dataset/train_target_3', target_3)\n",
    "np.save(PATH + 'dataset/test_input_3', test_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
